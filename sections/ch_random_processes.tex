\section{Definitions, notations, and terminology}
Recall: 
\begin{itemize}
    \item A random variable (\defref{def:random variable}) $\rv{x}$ maps outcomes to number.
    \item A random vector (\defref{def:multiple random variable}) $\mbfrv{x}$ maps outcomes to vectors.
\end{itemize}
\begin{mydefinition}[Random process]
    \label{def:random process}
    A random process (RP)\index{Random process (RP)} $\rv{x}(t)$, $t\in T$, maps outcomes to \emph{functions} of $t$.
\end{mydefinition}
\begin{myremark}
    Some remarks on random processes. 
    \begin{itemize}
        \item A RP is a family of RVs indexed by $t$.
        \item The \emph{range}\index{Range of random process} $I$ of $\rv{x}(t)$ is called the \emph{state space}\index{State space} of the process.
        \item A realization $x(t) = \rv{x}(s_{0}; t)$ (for a fixed $s_{0}$) of a RP $\rv{x}(t)$ is also called a \emph{sample path}\index{Sample path} or a \emph{sample function}\index{Sample function} of the process.
        \item If $I$ is a subset of $\rnums$ then the process is called a real RP\index{Real random process}. If $I$ is a subset of $\mbb{C}$ then the process is called a complex RP\index{Complex random process}
        \item Two stochastic processes $\rv{x}_{1}(t)$ ad $\rv{x}_{2}(t)$ are called \emph{equal}\index{Equal random processes} if their sample functions are always equal. That is,
        \begin{align}
            \rv{x}_{1}(s; t) &= \rv{x}_{2}(s; t), \qquad \forall t\in T, \forall s\in S.
        \end{align}
    \end{itemize}
\end{myremark}


\begin{table}[H]
    \begin{tabular}{l|ll}
        & \multicolumn{1}{c}{\textbf{$T$ discrete}}                                                                                    & \multicolumn{1}{c}{\textbf{$T$ continuous}}                                                                            \\ \cline{2-3} 
        \textbf{$I$ discrete}          & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Discrete-time random chain, or\\ discrete state random sequence\end{tabular}} & \begin{tabular}[c]{@{}l@{}}Continuous-time stochastic chain, or\\ continuous-time discrete random process\end{tabular} \\
        \textbf{$I$ continuous}        & \multicolumn{1}{l|}{Continuous-state random sequence}                                                                        & Continuous-time continuous-state process                                                                              
    \end{tabular}
    \caption{Types of random processes.}
    \label{tab:types of random processes}
    \index{Discrete-time random chain}
    \index{Discrete state random sequence}
    \index{Continuous-time stochastic chain}
    \index{Continuous-time discrete random process}
    \index{Continuous-state random sequence}
    \index{Continuous-time continuous-state}
    \index{Continuous-time continuous-state process}
\end{table}

\begin{mydefinition}[First order distribution of a RP]
    \index{First order distribution of a RP}
    The \emph{first order} distribution of the RV $\rv{x}(t)$ is  
    \begin{align}
        \cdf{x; t} &= \underbrace{\prob{\rv{x}(t) \leq x)}}_{\text{CDF of } \rv{x}(t)}
    \end{align}
\end{mydefinition}
\begin{mydefinition}[First order statistic of a RP]
    \index{First order statistic of a RP}
    There are two cases.
    \begin{enumerate}
        \item If $\rv{x}(t)$ is a continuous RV, then the \emph{first order density}\index{First order density} of the RP $\rv{x}(t)$ is 
        \begin{align}
            \pdf{x; t} &= \td{}{x}\cdf{x; t}, \qquad \forall t\in T.
        \end{align}
            
        \item If $\rv{x}(t)$ is a discrete RV, then the \emph{first order probability mass function}\index{First order probability mass function} is
        \begin{align}
            \pmf{x; t} &= \prob{\rv{x}(t) = x}, \qquad \forall t\in T.
        \end{align}
    \end{enumerate}
\end{mydefinition}

\begin{mydefinition}[Second order distribution of a RP]
    \index{Second order distribution of a RP}
    The \emph{second order distribution of a RP} $\rv{x}(t)$ is
    \begin{align}
        \cdf{x_{1}, x_{2}; t_{1}, t_{2}} &= \underbrace{\prob{\rv{x}(t_{1}) \leq x_{1}, \rv{x}(t_{2})\leq x_{2}}}_{\text{JCDF of } \rv{x}(t_{1}), \rv{x}(t_{2})}
    \end{align}
\end{mydefinition}
\begin{mydefinition}[Second order density of a RP]
    \index{Second order density of a RP}
    There are two cases.
    \begin{enumerate}
        \item If $\rv{x}(t_{1})$ and $\rv{x}(t_{2})$ are continuous in their domain. Then, the \emph{second order density} of a RP $\rv{x}(t)$ is 
        \begin{align}
            \pdf{x_{1}, x_{2}; t_{1}, t_{2}} &= \f{\partial^{2}}{\partial x_{1}\partial x_{2}} \cdf{x_{1}, x_{2}; t_{1}, t_{2}}, \qquad \forall t\in T.
        \end{align}

        \item If $\rv{x}(t_{1})$ and $\rv{x}(t_{2})$ are discrete, then the \emph{second order probability mass function} of the RP $\rv{x}(t)$ is
        \begin{align}
            \pmf{x_{1}, x_{2}; t_{1}, t_{2}} &= \prob{\rv{x}(t_{1}) = x_{1}, \rv{x}(t_{2}) = x_{2}}, \qquad \forall t\in T.
        \end{align}
    \end{enumerate}
\end{mydefinition}

\begin{mydefinition}[Higher order statistics of a RP]
    For the $n$ RVs $\rv{x}(t_{1}), \ldots, \rv{x}(t_{n})$, 
    \begin{enumerate}
        \item the $n$th order distribution of $\rv{x}(t)$ is 
        \begin{align}
            \cdf{x_{1},\ldots, x_{n}; t_{1}, \ldots, t_{n}},
        \end{align}
        \item the $n$th order density of $\rv{x}(t)$ is
        \begin{align}
            \pdf{x_{1},\ldots, x_{n}; t_{1}, \ldots, t_{n}},
        \end{align}
        \item the $n$th order probability mass function of $\rv{x}(t)$ is         
        \begin{align}
            \pmf{x_{1}, \ldots, x_{n}; t_{1}, \ldots, t_{n}}.
        \end{align}
    \end{enumerate}
\end{mydefinition}

\begin{mydefinition}[Mean of a random process]
    \index{Mean of a RP}  
    The \emph{mean} $\eta(t)$ of a RP $\rv{x}(t)$ is 
    \begin{align}
        \eta(t) &= \expect{\rv{x}(t)}\\
        &= \int x\pdf{x; t}\dee x.
    \end{align}
    Note that the mean is a function of time.
\end{mydefinition}

\begin{mydefinition}[Autocorrelation]
    \index{Autocorrelation of a RP}
    The \emph{autocorrelation} $\acor{t_{1}}[t_{2}]$ of a random process $\rv{x}(t)$ is
    \begin{align}
        \acor{t_{1}}[t_{2}] &= \expect{\rv{x}(t_{1})\rv{x}(t_{2})}\\
        &= \int\int x_{1}x_{2}\pdf{x_{1}, x_{2}; t_{1}, t_{2}}\dee x_{1} \dee x_{2}.
    \end{align}
\end{mydefinition}
\begin{mydefinition}[Autocovariance]
    \index{Autocovariance of a RP}
    The \emph{autocovariance} $\acov{t_{1}}[t_{2}]$ of a random process $\rv{x}(t)$ is
    \begin{align}
        \acov{t_{1}}[t_{2}] &= \expect{\left(\rv{x}(t_{1}) - \eta(t_{1})\right)\left(\rv{x}(t_{2}) - \eta(t_{2})\right)}\\
        &= \acor{t_{1}}[t_{2}] - \eta(t_{1})\eta(t_{2}).        
    \end{align}
    For $t_{1} = t_{2}$, 
    \begin{align}
        \acov{t}[t] &= \var{\rv{x}(t)}.
    \end{align}
\end{mydefinition}

\begin{mydefinition}[Independent increments]
    \index{Independent increments of a RP}
    A RP $\rv{x}(t)$ is said to have \emph{independent increments} if for any $k$ and any choice of $t_{1} < t_{2} < \ldots < t_{k}$, the RVs 
    \begin{align}
        \rv{x}(t_{2}) - \rv{x}(t_{1}), \ldots, \rv{x}(t_{k}) - \rv{x}(t_{k-1})
    \end{align}
    are independent RVs.
\end{mydefinition}

\begin{mydefinition}[Markov RP]
    \index{Markov RP}
    A RP $\rv{x}(t)$ is said to be \emph{Markov} if the future of the process given the present is independent of the past. That is, for any $k$ and choice of 
    \begin{align}
        \underbrace{t_{1} < \ldots < t_{k-1}}_{\text{past}} < \underbrace{t_{k}}_{\text{present}} < \underbrace{t_{k+1}}_{\text{future}},
    \end{align}
    the following relation holds
    \begin{align}
        &\pdf[x(t_{k+1})]{x_{k+1}\mid \rv{x}(t_{k})=x_{k}, \rv{x}(t_{k-1})=x_{k-1}, \ldots, \rv{x}(t_{1})=x_{1}}\\
        &\qquad\quad= \pdf[x(t_{k+1})]{x_{k+1} \mid \rv{x}(t_{k})=x_{k}}.
    \end{align}
\end{mydefinition}
%
\begin{mydefinition}[Independent RPs]
  \index{Independent RPs}
  Two RPs $\rv{x}(t)$ and $\rv{y}(t)$ are called \emph{independent} if the sets $(\rv{x}(t_{1}), \ldots, \rv{x}(t_{n}) )$ and $\left( \rv{y}(t_{1}^{\prime}), \ldots, \rv{y}(t_{n}^{\prime}) \right))$ are mutually independent for any $n$, $n^{\prime}$ and choice of pointes $t_{1}, \ldots, t_{n}$ and $t^{\prime}_{1}, \ldots, t^{\prime}_{n^{\prime}}$.
\end{mydefinition}

\begin{mydefinition}[White noise]
    \index{White noise}
    A RP $\rv{x}(t)$ is called \emph{white noise} if its values $\rv{x}(t_{1})$ and $\rv{x}(t_{2})$ are uncorrelated for any $t_{1}\neq t_{2}$ . That is,
    \begin{align}
        \acov{t_{1}}[t_{2}] &= 0, \qquad \forall t_{1}\neq t_{2}.
    \end{align}
\end{mydefinition}
\begin{myremark}
    The autocovariance of a non-trivial white-noise process must be of the form
    \begin{align}
        \acov{t_{1}}[t_{2}] &= q(t_{1})\delta (t_{1} - t_{2}), \qquad q(t)\geq 0.
    \end{align}
\end{myremark}

\begin{mydefinition}[Strictly white noise]
    \index{Strictly white noise}  
    A RP $\rv{x}(t)$ is called \emph{strictly white noise} if its values $\rv{x}(t_{1})$ and $\rv{x}(t_{2})$ are \emph{independent} for all $t_{1}\neq t_{2}$.
\end{mydefinition}

\begin{mydefinition}[Normal (Gaussian) RPs]
    \index{Normal RPs}\index{Gaussian RPs}
    A RP $\rv{x}(t)$ is called \emph{Normal (Gaussian)} if for any $k$ and any choice of $t_{1}, \ldots, t_{k}$, the RVs $\rv{x}(t_{1}), \ldots, \rv{x}(t_{k})$ are jointly normal. 
\end{mydefinition}
\begin{myremark}[Gaussian RPs]
    The statistics of a Normal RP are completely determined by its mean $\eta(t)$ and autocovariance $\acov{t_{1}}[t_{2}]$.
\end{myremark}