\section{Bernoulli distribution}
From \cite{psaromiligkos_slides_2019}.
\begin{mydefinition}[Bernoulli drandom variable]
    A random variable $\rv{x}$ is called Bernoulli with parameter $p$ if there exists an event $A$ with probability $p = \prob{A}$ such that
    \begin{align}
        x(s) &=
        \begin{cases}
            1, & s\in A\\
            0, & s\notin A,
        \end{cases}
    \end{align}
    where $s\in S$.
\end{mydefinition}
\begin{myremark}
    Some remarks on Bernoulli random variables.
    \begin{itemize}
        \item $\rv{x}$ is a \emph{discrete} random variable with range $\mc{R}_{x} = \left\{ 0, 1\right\}$.
        \item PMF of $\rv{x}$ 
        \begin{align}
            \pmf{0} &= \prob{\rv{x} = 0} = \prob{A^{\comp}} = 1 - p = q\\
            \pmf{1} &= \prob{\rv{x} = 1} = \prob{A} = p\\
            \pmf{x} &= 0 \qquad \text{if } x\notin \left\{ 0,1\right\}.
        \end{align}
    \end{itemize}
\end{myremark}

\section{Geometric random variables}
From \cite{psaromiligkos_slides_2019}.
\begin{mydefinition}[Geometric random variable]
    A geometric random variable is a discrete random variable with a countable infinite set of possible values. The set of possible values for the geometric random variable $\rv{x}$ is
    \begin{align}
        \mc{R}_{x} &= \left\{ 1, 2, \ldots\right\} = \mbb{N}.
    \end{align}
\end{mydefinition}
\begin{mytheorem}
    [PMF of geometric random variables]    
    Let $\rv{x}$ be a geometric random variable with parameter $p$. The PMF of $\rv{x}$ is then
    \begin{align}
        \pmf{x} &= 
        \begin{cases}
            \left( 1 - p \right)^{x-1}p, &\qquad x = 1, 2, \ldots,\\
            0, & \text{otherwise}.
        \end{cases}
    \end{align}
\end{mytheorem}

\section{Binomial random variable}
\begin{mydefinition}[Binomial random variable]
    Consider a sequence of $n$ identical and independent Bernoulli trials with probability of success $p$. The random variable $\rv{x}$ defined by
    \begin{align}
        \rv{x} &= \text{number of successes in the $n$ trials}
    \end{align}
    is called binomial with parameters $n$ and $p$, or simply $B(n,p)$.
\end{mydefinition}
\begin{myremark}
    Some remarks on Binomial random variables.
    \begin{itemize}
        \item Set of possible values of $\rv{x}$:
        \begin{align}
            \mc{R}_{x} &= \left\{ 0, 1, \ldots, n\right\}.
        \end{align}
        \item Notation:
        \begin{align}
            \rv{x} \sim B(n, p).
        \end{align}
        \item Basic examples
        \begin{itemize}
            \item Number of heads in a sequence of 10 independent flips of a coin.
            \item Number of defective IC chips in a production sample of size $n$.
        \end{itemize}
    \end{itemize}
\end{myremark}
\begin{mytheorem}
   [PMF of a Binomial random variable]    
   Let $\rv{x}\sim B(n,p)$. The PMF of $\rv{x}$ is
   \begin{align}
       \pmf{x} &= 
       \begin{cases}
           \binom{n}{k} p^{x}(1-p)^{n-x}, &x = 0, 1, \ldots, n\\
           0, &\text{otherwise}.
       \end{cases}
   \end{align}
\end{mytheorem}


\section{Poisson distribution}
A discrete random variable $\rv{x}$ is called Poisson with parameter $\lambda > 0$ if its PMF has the form
\begin{align}
    \pmf{x} &= 
    \begin{cases}
        \f{\lambda^{x}}{x!}e^{-\lambda}, &x\in\mbb{N}_{0}\\
        0, &\text{otherwise},
    \end{cases}
\end{align}
where $\mbb{N}_{0} = \left\{ 0, 1, \ldots\right\}$.

\section{Uniform random variables}
\begin{mydefinition}[Uniform random variable]
    A continuous random variable $\rv{x}$ is called uniform over the interval $(a,b)$, written as $\rv{x}\sim U(a,b)$, where $a<b$ are real numbers, if its PDF is
    \begin{align}
        \pdf{x} &= 
        \begin{cases}
            \f{1}{b-a}, & a < x < b,\\
            0, &\text{otherwise}.
        \end{cases}
    \end{align}
\end{mydefinition}

\begin{mytheorem}
   [Unoform random variable CDF]    
     The CDF is given by
     \begin{align}
         \cdf{x} &=
         \begin{cases}
             0, & x \leq a,\\
             \f{x-a}{b-a}, & a\leq x\leq b,\\
             1& x \geq b.
         \end{cases}
     \end{align}
\end{mytheorem}

\section{Exponential random variable}
\begin{mydefinition}[Exponential random variable]
    A continuous random variable $\rv{x}$ is called exponential with parameter $\lambda > 0$ if its PDF is given by
    \begin{align}
        \pdf{x} &= 
        \begin{cases}
            \lambda e^{-\lambda x}, &x\geq 0,\\
            0, &x<0.
        \end{cases}
    \end{align}
\end{mydefinition}
\begin{mytheorem}
   [Exponential random variable CDF]    
   The CDF is given by
   \begin{align}
       \cdf{x} &= 
       \begin{cases}
           1 - e^{-\lambda x}, & x > 0,\\
           0, x \leq 0.
       \end{cases}
   \end{align}  
\end{mytheorem}


\section{Gaussian random variables}
\begin{mydefinition}[Gaussian (normal) distribution]
    A random vector $\mbfrv{x}$ is called a \emph{Gaussian (or normal) random vector} with mean $\mbs{\mu}$ and covariance matrix $\mbf{C}$ if it follows a \emph{Gaussian (or normal) distribution} given by
    \begin{align}
        \pdf{\mbf{x}} &= 
        \f{1}{\sqrt{(2\pi)^{n}}\det\mbf{C}} e^{-\onehalf (\mbf{x}-\mbs{\mu})^{\trans}\mbf{C}\inv(\mbf{x}-\mbs{\mu})}\\
        &= \mc{N}\left( \mbs{\mu}, \mbf{C} \right).
    \end{align}
    The $n$ elements of $\mbf{x}$, $\stringRVs{x}{n}$ are called \emph{jointly normal random variables}.
\end{mydefinition}

\begin{mydefinition}[Standard Normal Distribution]
    \label{def:standard normal distribution}
    A \emph{standard normal distribution} is a normal distribution with a special case of $\mbs{\mu}=\mbf{0}$ and $\mbf{C}=\eye$. That is,
    \begin{align}
        \pdf{\mbf{x}} &= \mc{N}\left( \mbf{0}, \eye \right).
    \end{align}
\end{mydefinition}

\subsection{Properties of Gaussian random variables}
\begin{mytheorem}
   [Linear combination of normal RVs]    
   A random vector $\mbfrv{x}$ is called a Gaussian (or normal) random vector if and only if the random variable $\mbf{a}^{\trans}\mbfrv{x}$ is a Gaussian random variable for any vector $\mbf{a}$.           
\end{mytheorem}
\begin{myremark}[Properties of Gaussian random variables]
    Here are some properties of Gaussian random variables. 
    \begin{enumerate}
        \item A Gaussian random vector consists of marginally Gaussian random variables.
        \item However, marginally Gaussian random variables are \emph{not} necessarily jointly Gaussian.\footnote{That is, if you take 3 Gaussian random variables, then they are not necessarily jointly Gaussian. Unless they are independent.}
    \end{enumerate}
\end{myremark}

\begin{mydefinition}[Complex Gaussian radnom vector]
    A complex random vector 
    \begin{align}
        \mbfrv{z} =
        \bbm \rv{x}_{1}+\jmath\rv{y}_{1} \\ \vdots\\\rv{x}_{n}+\jmath\rv{y}_{n} \ebm
    \end{align}
    is a \emph{complex Gaussian random vector} if and only if the real random variables $\rv{x}_{1},\rv{y}_{1},\ldots,\rv{x}_{n},\rv{y}_{n}$ are jointly normal.
\end{mydefinition}

\begin{mytheorem}
   [Characteristic function of normal random variables]    
    If the random variables $\stringRVs{x}{n}$ are jointly Normal with mean $0$ and covariance matrix $\mbf{C}$. Then, their characteristic function is     
    \begin{align}
        \Phi_{\mbf{x}}(\Omega) &= e^{-\onehalf\Omega^{\trans}\mbf{C}\Omega}.
    \end{align}
\end{mytheorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To add:
%   Lemma A.1 and A.2 from Farrel.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Chi-squared distribution}
\begin{mydefinition}[Chi-squared distribution]
    \label{def:chi squared distribution}
    If $\rv{x}_{1}, \ldots, \rv{x}_{n}$ are \emph{\textbf{independent} standard normal variables} (check Definition~\ref{def:standard normal distribution}), then the sum of their squares 
    \begin{align}
        \rv{y} &= \sum_{k=1}^{n}\rv{x}_{k}^{2} \in \rnums
    \end{align}
    is distributed according to the Chi-square distribution with $n$ degrees of freedom, denoted by $\chi_{n}^{2}$. That is,
    \begin{align}
        \rv{y}\sim\chi^{2}_{n}.
    \end{align}
\end{mydefinition}
%
\begin{myremark}
    If we have a random vector $\mbfrv{x}\in\rnums^{n}$ with mean $\mbs{\mu}_{x}=\mbf{0}$ and a unit covariance $\mbf{C}=\eye$, then the sum of squares    
    \begin{align}
        \rv{y} &= \mbfrv{x}^{\trans}\mbfrv{x}\\        
    \end{align}
    follows a Chi-squared distribution with $n$ degrees of freedom. That is
    \begin{align}
        \rv{y} &\sim\chi^{2}_{n}.
    \end{align}
\end{myremark}
%
\begin{myremark}
    MATLAB has a built-in inverse CDF function called \texttt{chi2inv(p,nu)} that evaluates the inverse CDF of a Chi-squared distribution at the probability value $p\in[0,1]$ and the degrees of freedom $\nu$ (which we denote as $n$ in Definition~\ref{def:chi squared distribution}). 
\end{myremark}

\subsection{Relation to the Mahalanobis distance}
In estimation problems, the Mahalanobis distance is often encountered. Especially when testing for consistency of the estimates.
\begin{mydefinition}[Mahalanobis distance]
    The \emph{Mahalanobis distance} $D_{M}\in\rnums$ is defined as 
    \begin{align}
        D_{M} &\triangleq \sqrt{\mbf{r}^{\trans}\mbs{\Sigma}^{\inv}\mbf{r}},
    \end{align}
    where $\mbf{r}\in\rnums^{n}$ is a sample of 
    \begin{align}
        \mbfrv{r} \sim\mc{N}\left( \mbf{0}, \mbs{\Sigma} \right).
    \end{align}
    It is a \emph{Euclidean distance} normalized by \emph{uncertainty}.
\end{mydefinition}
%%
\begin{example}
    Consider a random variable $\mbfrv{x}\in\rnums^{n}$ distributed according to $\mbfrv{x}\sim\mc{N}\left(\mbs{\mu}_{x},\mbs{\Sigma}_{x}\right)$. Say a realization $\mbf{x}\in\rnums^{n}$ is sampled from the distribution. Then the Mahalanobis distance
    \begin{align}
        D_{M} = \sqrt{\left(\mbf{x}-\mbs{\mu}_{x}\right)^{\trans}\mbs{\Sigma}\inv\left( \mbs{x}-\mbs{\mu}\right)}
    \end{align}
    is a measure of how far off the realization $\mbf{x}$ is from the mean $\mbs{\mu}_{x}$.
    \triqed
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Relation to the Chi-distribution test
\begin{mytheorem}   
    The squared Mahalanobis distance follows a Chi-squared distribution. That is, 
    \begin{align}
        \rv{D_{M}}^{2}\sim\chi^{2}_{n}.
    \end{align}
\end{mytheorem}
\begin{proof}
    Given a random variable $\mbfrv{r}\in\rnums^{n}$ that follows the distribution $\mbfrv{r}\sim\left(\mbf{0},\mbs{\Sigma} \right)$, then the Mahalanobis distance squared is a random variable. Specifically,
    \begin{align}
        \label{eq:Mahalanobis distance square eq 1}
        \rv{D}_{M}^{2} &= \mbfrv{r}^{\trans}\mbs{\Sigma}\inv\mbfrv{r}.
    \end{align}
    Since the covariance matrix $\mbs{\Sigma}$ is symmetric positive definite, then its eigenvectors are orthonormal and the eigenvalues are positive. Therefore, \eqref{eq:Mahalanobis distance square eq 1} can be written as
    \begin{align}
        \rv{D}_{M}^{2} 
        &= \mbfrv{r}^{\trans}\mbs{\Sigma}\inv\mbfrv{r}\\
        &= \mbfrv{r}^{\trans}\left(\mbf{U}\mbs{\Lambda}\mbf{U}^{\trans} \right)\inv\mbfrv{r}\\
        &= \mbfrv{r}^{\trans}\mbf{U}\mbs{\Lambda}\inv\mbf{U}^{\trans}\mbfrv{r}\\
        &= \mbfrv{r}^{\trans}\mbf{U}\mbs{\Lambda}^{-\f{1}{2}}\mbs{\Lambda}^{-\f{1}{2}}\mbf{U}^{\trans}\mbfrv{r}\\
        &= \left(\mbs{\Lambda}^{-\f{1}{2}}\mbf{U}^{\trans}\mbfrv{r}\right)^{\trans}\left(\mbs{\Lambda}^{-\f{1}{2}}\mbf{U}^{\trans}\mbfrv{r}\right)\\
        &= \mbfrv{y}^{\trans}\mbfrv{y},
    \end{align}
    where
    \begin{align}
        \mbfrv{y} &= \mbs{\Lambda}^{-\f{1}{2}}\mbf{U}^{\trans}\mbfrv{r}\in\rnums^{n}.
    \end{align}
    The next step is to check whether $\mbfrv{y}$ is a standard normal variable. First, let's check the mean.
    \begin{align}
        \mbs{\mu}_{y} 
        &= \expect{\mbfrv{y}} \\
        &= \mbs{\Lambda}^{-\f{1}{2}}\mbf{U}^{\trans}\underbrace{\expect{\mbfrv{r}}}_{\mbf{0}}\\
        &= \mbf{0}.
    \end{align}
    The second condition to check is whether the covariance of $\mbfrv{y}$ is the identity matrix or not.
    \begin{align}
        \cov{\mbfrv{y}} 
        &= \expect{\mbfrv{y}\mbfrv{y}^{\trans}}\\
        &= \mbs{\Lambda}^{-\f{1}{2}}\mbf{U}^{\trans}\underbrace{\expect{\mbfrv{r}\mbfrv{r}^{\trans}}}_{\mbs{\Sigma}}\mbf{U}\mbs{\Lambda}^{-\f{1}{2}}\\
        &= \mbs{\Lambda}^{-\f{1}{2}}\mbf{U}^{\trans}\underbrace{\mbs{\Sigma}}_{\mbf{U}\mbs{\Lambda}\mbf{U}^{\trans}}\mbf{U}\mbs{\Lambda}^{-\f{1}{2}}\\
        &= \mbs{\Lambda}^{-\f{1}{2}}\cancelto{\eye}{\mbf{U}^{\trans}\mbf{U}}\mbs{\Lambda}\cancelto{\eye}{\mbf{U}^{\trans}\mbf{U}}\mbs{\Lambda}^{-\f{1}{2}}\\
        &= \mbs{\Lambda}^{-\f{1}{2}}\mbs{\Lambda}\mbs{\Lambda}^{-\f{1}{2}}\\
        &= \eye.
    \end{align}
    Thus, $\mbfrv{y}$ is a standard normal random variable. Therefore, according to Definition~\ref{def:chi squared distribution}, $\rv{D}_{M}^{2}$ follows a Chi-squared distribution with $n$ degrees of freedom.
\end{proof}
