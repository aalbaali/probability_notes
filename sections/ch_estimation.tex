\section{Motivation}
Let $\rv{x}$ be a random variable whose CDF, $\cdf{x}$, \emph{is unknown}. The goal is estimate $\cdf{x}$ from $n$ observations of $\rv{x}$, $x_{1}, \ldots, x_{n}$.

\begin{mydefinition}[Empirical CDF]
    The empirical CDF of $\rv{x}$ from the observations $x_{1}, \ldots, x_{n}$ is
    \begin{align}
        \hat{\cdf{}}(x) &= \f{1}{n} \times \left( \# \text{ of observations } x_{i} \leq x \right).
    \end{align}
\end{mydefinition}
The empirical CDF $\hat{\cdf{}}(x)$ is the CDF of a discrete random variable with PMF
\begin{align}
    \hat{\pmf{}}_{n}(x) &= 
    \begin{cases}
        \f{1}{n}, & x = x_{i}, \quad i = 1,\ldots, n,\\
        0 & \text{Otherwise}.
    \end{cases}
\end{align}
The PDF is then given by
\begin{align}
    \label{eq:empirical pdf constant weight}
    \hat{\pdf{}}_{n}(x) &= \sum_{i=1}^{n}\f{1}{n}\delta(x-x_{i}).
\end{align}

The empirical PDF $\hat{\pdf{}}(x)$ in \eqref{eq:empirical pdf constant weight} assigns constant weights to every observation. The empirical PDF can be generalized by giving different weights to different observations. Specifically,
\begin{align}
    \hat{\pdf{}}_{n}(x) &= \sum_{i=1}^{n}\f{1}{n}k(x-x_{i}),
\end{align}
where $k(\cdot)$ is the kernel function such that it is
\begin{enumerate}
    \item symmetric around 0, and
    \item $\int_{-\infty}^{\infty}k(x)\dee x = 1$.
\end{enumerate}
A common choice is the Gaussian kernel. That is, a PDF of $\mc{N}(0,1)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Parameter estimation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem statement}
Let $\mbfrv{x}$ be a random variable. Furthermore, let $\pdf{\mbf{x}; \mbs{\theta}}$ be the PDF of $\mbfrv{x}$ of \emph{known form} (e.g., Gaussian distribution) but depending on \emph{unknown} parameters $\mbs{\theta}$ (e.g., mean and covariance of a Gaussian distribution).
\begin{example}
    A Gaussian random variable has a PDF parameterized w.r.t. to the mean ${\mu}$ and the variance $\sigma^{2}$. Thus, the parameters could be $\mbs{\theta} = \bbm \mu, \sigma^{2} \ebm$.
\end{example}

The goal is to estimate $\mbs{\theta}$ from $n$ observations of $\mbfrv{x}$: $\mbf{x}_{1}, \ldots, \mbf{x}_{n}$. 

Define the data vector 
\begin{align}
    \mbf{X} &= \bbm \mbf{x}_{1}\\\vdots\\\mbf{x}_{n} \ebm.
\end{align}
The data vector $\mbf{X}$ is a realization of the random vector 
\begin{align}
    \mbfrv{X} &= \bbm \mbfrv{x}_{1} \\ \vdots \\ \mbfrv{x}_{n} \ebm,
\end{align}
where $\mbfrv{x}_{i}$, $i=1,\ldots, n$ are i.i.d.

\subsection*{Approaches}
There are two approaches to parameter estimation.
\begin{enumerate}
    \item \emph{Point estimation}: identify a function $g(\cdot)$ of the sample such that $g(\mbfrv{x}_{1},\ldots, \mbfrv{x}_{n})$ is a good (in some sense) approximation of $\mbs{\theta}$.
    \item \emph{Interval estimation}: identify two functions $g_{1}(\cdot)$ and $g_{2}(\cdot)$ of the sample, such that
    \begin{align}
        \prob{g_{1}(\mbfrv{x}_{1},\ldots,\mbfrv{x}_{n})<\mbs{\theta}<g_{2}(\mbfrv{x}_{1},\ldots,\mbfrv{x}_{n})} = \gamma,
    \end{align}
    where $\gamma$ is the confidence interval.
\end{enumerate}

\section{Definitions and terminology}
\begin{mydefinition}[Estimator]
    The estimator of $\mbs{\theta}$ is given in the form
    \begin{align}
        \hat{\mbsrv{\theta}} &= g(\mbfrv{x}_{1}, \ldots, \mbfrv{x}_{n}).
    \end{align}

    The estimator models the behaviour of the estimation method over all possible samples.
\end{mydefinition}
\begin{mydefinition}[Estimate of $\mbs{\theta}$]
    The \emph{deterministic} quantity $g(\mbf{x}_{1},\ldots,\mbf{x}_{n})$. That is, the the output of $\hat{\mbsrv{\theta}}$ for the realizations $\mbf{x}_{1}, \ldots, \mbf{x}_{n}$.
\end{mydefinition}
\begin{mydefinition}[Statistic]
    Any function of $\mbfrv{x}_{1}, \ldots, \mbfrv{x}_{n}$.
\end{mydefinition}
\begin{mydefinition}[Bias]
    The estimator $g(\mbfrv{x}_{1}, \ldots, \mbfrv{x}_{n})$ is called
    \begin{itemize}
        \item \emph{unbiased} if 
        \begin{align}
            \expect{g(\mbfrv{x}_{1},\ldots, \mbfrv{x}_{n})} &= \mbs{\theta}.
        \end{align}
        \item Otherwise, it's called \emph{biased} with bias
        \begin{align}
            \expect{g(\mbfrv{x}_{1},\ldots, \mbfrv{x}_{n})} - \mbs{\theta}.
        \end{align}
    \end{itemize}
\end{mydefinition}

\begin{mydefinition}[Asymptic unbias]
    A biased estimator for which
    \begin{align}
        \expect{g(\mbfrv{x}_{1},\ldots,\mbfrv{x}_{n})} \overset{N\to\infty}{\to} \mbs{\theta}
    \end{align}
    is called \emph{asymptotically unbiased}.
\end{mydefinition}

\begin{mydefinition}[Mean-squared best estimator]
   For a fixed $n$, the estimator that minimizes the mean-squared (MS) error 
   \begin{align}
       \mbf{e}_{n} &= \expect{\abs{g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})^{\trans}g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})}}
   \end{align}
   is called the \emph{best} estimator (in the mean-squared sense).
\end{mydefinition}

\begin{mydefinition}[Consistency]
    The estimator $g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})$ is called \emph{consistent} if 
    \begin{align}
        \lim_{n\to\infty} \prob{\abs{g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})}>\mbs{\epsilon}} &= 0, \forall \mbs{\epsilon}>\mbf{0},
    \end{align}
    or
    \begin{align}
        \lim_{n\to\infty} \prob{\abs{g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})}<\mbs{\epsilon}} &= 1, \forall \mbs{\epsilon}>\mbf{0}.
    \end{align}
\end{mydefinition}

\begin{mytheorem}
    [Sufficient condition for consistency]    
    Let $g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})$ be an estimator of $\mbs{\theta}$. Then, if
    \begin{align}
        \lim_{n\to\infty} \expect{\abs{g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})^{\trans}g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})}} &= \mbf{0},
    \end{align}
    then $g(\mbfrv{x}_{1},\ldots\mbfrv{x}_{n})$ is a \emph{consistent estimator} of $\mbs{\theta}$.

    The converse is not true in general. That is, this is a sufficient but not necessary condition.
\end{mytheorem}


\begin{mytheorem}
   [Sample mean estimator]    
     Let $\mbfrv{x}_{1},\ldots, \mbfrv{x}_{n}$ be a random sample of size $n$ and let $\mbfrv{x}_{1},\ldots, \mbfrv{x}_{n}$ be \emph{uncorrelated}. The \emph{sample mean estimator}
     \begin{align}
         \hat{\mbsrv{\mu}}_{n} &= \f{1}{n}\sum_{i=1}^{n}\mbfrv{x}_{i}
     \end{align}
     is
     \begin{itemize}
         \item an \emph{unbiased estimator} of $\mbs{\mu}$;
         \item a \emph{consistent estimator} of $\mbs{\mu}$.
     \end{itemize}
\end{mytheorem}

\begin{mytheorem}
   [Variance estimaton]    
   Consider the sample average estimators of the mean and variance of Gaussian random variable $\rv{x}$ from a random sample $\rv{x}_{1},\ldots, \rv{x}_{n}$ of size $n$:
   \begin{align}
       \hat{\rv{\mu}}_{n} &= \f{1}{n}\sum_{i=1}^{n}\rv{x}_{i},\\
       \hat{\rv{\sigma}}_{n}^{2} &= \f{1}{n} \sum_{i=1}^{n}\left( \rv{x}_{i}-\hat{\rv{\mu}}_{n} \right)^{2}.
   \end{align}
   The random variables $\hat{\rv{\mu}}_{n}$ and $\hat{\rv{sigma}}_{n}^{2}$ are independent.
\end{mytheorem}

\section{Method of moments}
Recall that moments are defined in Def.~\ref{def:moments single rv} and Def.~\ref{def:moments multiple rv}. 
The $m$-th moment estimator is given by
\begin{align}
    \hat{m}_{m}(\mbf{X}) &= 
    \f{1}{n}\sum_{i}^{n}x_{i}^{m}.
\end{align}
Let $\mbs{\theta}\in\rnums^{k}$ be of size $k$ and let 
\begin{align}
    \mbf{X} &= \bbm x_{1} \\\vdots\\x_{n} \ebm 
\end{align}
be the samples of the random variable $\rv{x}$. Then, 
the idea of method of moments is to estimate the first $k$ moments, thus getting $k$ equations, and then solving the $k$ equations for $\mbs{\theta}$.

\begin{example}
    Estimate the mean $\mu$ and variance $\sigma^{2}$ of a random variable $\rv{x}$ from a random sample $\mbfrv{X} = \bbm \rv{x}_{1}&\ldots&\rv{x}_{n} \ebm^{\trans}$.

    The moments are given by
    \begin{alignat}{2}
        m_{1}(\mu, \sigma^{2}) &= \expect{\rv{x}} &&= \mu, \\
        m_{2}(\mu, \sigma^{2}) &= \expect{\rv{x}^{2}} &&= \mu^{2} + \sigma^{2}.
    \end{alignat}
    The sample average estimates of the moments are given by
    \begin{align}
        \hat{m}_{1}(\mbf{X}) &= \f{1}{n}\sum_{i=1}^{n}x_{i},\\
        \hat{x}_{2}(\mbf{X}) &= \f{1}{n}\sum_{i=1}^{n}x_{i}^{2}.
    \end{align}
    Thus, the parameter estimates can be computed by solving the system of equations
    \begin{align}
        \mu &= \hat{m}_{1}(\mbf{X})\\
        \mu^{2} + \sigma^{2} &= \hat{m}_{2}(\mbf{X}).
    \end{align}
    The solution is given by
    \begin{align}
        \hat{\mu} &= \hat{m}_{1}(\mbf{X}) \\
            &= \f{1}{n}\sum_{i=1}^{n}x_{i},\\
        \hat{\sigma}^{2} &= \hat{m}_{2}(\mbf{X}) - \hat{m}_{1}^{2}(\mbf{X})\\
        &= \f{1}{n}\sum_{i=1}^{n}x_{i}^{2}  - \left(\f{1}{n}\sum_{i=1}^{n}x_{i}\right)^{2}.
    \end{align}
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Maximum likelihood
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Maximum likelihood (ML) estimator}
\begin{mydefinition}[Likelihood]
    Given the observations $\mbfrv{x}_{1} = \mbf{x}_{1}', \ldots, \mbfrv{x}_{n}=\mbf{x}_{n}'$, the \emph{likelihood} of $\mbs{\theta}$ is the joint PDF of $\mbfrv{x}_{1},\ldots,\mbfrv{x}_{n}$ evaluated at the observed points
    \begin{align}
        \pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}} &= \pdf{}_{\mbf{x}}(\mbf{x}_{1}';\mbs{\theta})\cdots\pdf{}_{\mbf{x}}(\mbf{x}_{n}';\mbs{\theta}).
    \end{align}
\end{mydefinition}
\begin{myremark}
    \begin{itemize}
        \item The likelihood $\pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}}$ is treated as a function of $\mbs{\theta}$.
        \item It is not a PDF. That is,
        \begin{align}
            \int_{-\infty}^{\infty}\pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}}\dee\mbs{\theta}\neq 1.
        \end{align}
    \end{itemize}
\end{myremark}

\begin{mydefinition}[Maximum-likelihood estimator]
    The maximum-likelihood (ML) estimate of $\mbs{\theta}$, $\hat{\mbs{\theta}}_{\mathrm{ML}}$, is the value of $\mbs{\theta}$ that maximizes $\pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}}$. That is,
    \begin{align}
        \hat{\mbs{\theta}}_{\mathrm{ML}} &= \argmax_{\mbs{\theta}\in\rnums^{k}} \pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}}.
    \end{align}
\end{mydefinition}

\begin{myremark}
    \begin{itemize}
        \item Log-likelihood function 
        \begin{align}
            L(\mbf{x}_{1}',\ldots,\mbf{x}_{n}';\mbs{\theta}) = \log \pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}}.
        \end{align}
        \item The ML estimate is 
        \begin{align}
            \hat{\mbs{\theta}}_{\mathrm{ML}} 
            &= \argmax_{\mbs{\theta}\in\rnums^{k}} \pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}}\\
            &= \argmax_{\mbs{\theta}\in\rnums^{k}} \log \pdf{\mbf{x}_{1}',\ldots, \mbf{x}_{n}';\mbs{\theta}}\\
            &= \argmax_{\mbs{\theta}\in\rnums^{k}} L(\mbf{x}_{1}',\ldots,\mbf{x}_{n}';\mbs{\theta}).
        \end{align}
    \end{itemize}
\end{myremark}

\subsection{Remarks}
\begin{enumerate}
    \item Let $\hat{\mbsrv{\theta}}_{\mathrm{ML}}$ be the ML estimator of $\mbs{\theta}$ and let $g(\mbs{\theta})$ be a function of $\mbs{\theta}$. Then, the ML estimator of $g(\mbs{\theta})$ is $g(\hat{\mbsrv{\theta}}_{\mathrm{ML}})$.
    
    \item Limitations of the ML estimation:
    \begin{enumerate}
        \item The ML estimate may not exist.
        \item The ML estimate may not be unique. 
    \end{enumerate}        
    
    \item Maximum likelihood estimate can be finding by minimizing the negative likelihood or negative log-likelihood.
\end{enumerate}

\section{Bayesian parameter estimation}
% \subsection*{Introduction and terminology}
In the previous sections, the approaches used were the \emph{classical} or \emph{frequentist} approaches. The classical approach
\begin{enumerate}
    \item treats the unknown parameter as deterministic parameter, and
    \item assumes no prior information on the unknown parameter was available.
\end{enumerate}

For Bayesian approach on the other hand
\begin{enumerate}
    \item assumes that unknown parameter is random with known pdf $\pdf{\mbs{\theta}}$, called the \emph{prior};
    \item the JPDF of random sample given $\mbsrv{\theta} = \theta$ (likelihood) is given by
    \begin{align}
        \pdf{\mbf{x}_{1},\ldots,\mbf{x}_{n}\middle|~ \mbs{\theta}};
    \end{align}
    \item The estimation problem becomes a prediction problem; predict $\mbsrv{\theta}$ from a correlated (with $\mbsrv{\theta}$) sample $\mbfrv{X}$.
\end{enumerate}

The mean-squared estimate $\hat{\mbs{\theta}}$ of $\mbsrv{\theta}$ given $\mbfrv{X}=\mbf{X}$ is
\begin{align}
    \label{eq:bayesian estimator E(theta|X)}
    \hat{\mbs{\theta}} &= \expect{\mbsrv{\theta}\middle|~\mbf{X}}\\
    &= \int_{-\infty}^{\infty}\mbs{\theta}\pdf{\mbs{\theta}\middle|~\mbf{X}}\dee\mbs{\theta},
\end{align}
where
\begin{align}
    \pdf{\mbs{\theta}\middle|~\mbf{X}} &= 
        \f{
            \pdf{\mbf{X}\middle|~\mbs{\theta}}\pdf{\mbs{\theta}}
        }{\pdf{\mbf{X}}}\\
    &=
        \f{
            \pdf{\mbf{X}\middle|~\mbs{\theta}}\pdf{\mbs{\theta}}
        }{
            \int_{-\infty}^{\infty}\pdf{\mbf{X}\middle|~\mbs{\theta}}\pdf{\mbs{\theta}}\dee\mbs{\theta}
        }.
\end{align}

\begin{myBlueBox}
    \textbf{Terminology}
    \begin{itemize}
        \item $\pdf{\mbs{\theta}}$: \emph{prior} pdf of $\mbsrv{\theta}$ (before observing the sample).
        \item $\pdf{\mbs{\theta}\middle|~\mbf{X}}$: \emph{posterior} pdf of $\mbs{\theta}$ (after observing the sample).
        \item In \cite{barfoot_state_2017}, the term $\pdf{\mbf{X}\middle|~\mbs{\theta}}$ is sometimes referred to as the likelihood of $\mbf{X}$ given $\mbs{\theta}$. This is not to be confused with the likelihood from the ML estimator. In estimation, the term $\pdf{\mbf{X}\middle|~\mbs{\theta}}$ can also be referred to as an \emph{observation model}. 
    \end{itemize}
\end{myBlueBox}

\begin{myremark}
    \begin{itemize}
        \item $\expect{\mbsrv{\theta}\middle|~\mbf{X}}$ from \eqref{eq:bayesian estimator E(theta|X)} is difficult to solve for two reasons. First, the denominator of the posterior pdf involves evaluating an integral over the entire sample space of $\mbs{\theta}$. Second, evaluating the expectation $\expect{\cdot}$ requires \emph{another} integral to be evaluated over the entire sample space.
        
        \item Maximum a-posteriori (MAP) estimate maximizes the \emph{posterior} pdf $\pdf{\mbs{\theta}\middle|~\mbf{X}}$ with respect to $\mbs{\theta}$. Since the denominator of the prior is constant, then it can be omitted. That is,
        \begin{align}
            \hat{\mbs{\theta}}_{\mathrm{MAP}} 
            &= \argmax_{\mbs{\theta}} \pdf{\mbs{\theta}\middle|~\mbf{X}}\\
            &= \argmax_{\mbs{\theta}} \f{\pdf{\mbf{X}\middle|~\mbs{\theta}}\pdf{\mbs{\theta}}}{\pdf{\mbf{X}}}\\
            &= \argmax_{\mbs{\theta}} \pdf{\mbf{X}\middle|~\mbs{\theta}}\pdf{\mbs{\theta}}.
        \end{align}
    \end{itemize}
\end{myremark}

\section{Maximum a-posteriori (MAP) estimator}
This estimator is an \emph{approximation} to the Bayes estimator discussed in the previous section. It is easier (less computationally intensive) to compute.
\begin{mydefinition}[Maximum a-posteriori estimator]
    Maximum a-posteriori (MAP) estimate maximizes the \emph{posterior} pdf $\pdf{\mbs{\theta}\middle|~\mbf{X}}$ with respect to $\mbs{\theta}$. Since the denominator of the prior is constant, then it can be omitted. That is,
        \begin{align}
            \hat{\mbs{\theta}}_{\mathrm{MAP}} 
            &= \argmax_{\mbs{\theta}} \pdf{\mbs{\theta}\middle|~\mbf{X}}\\
            &= \argmax_{\mbs{\theta}} \f{\pdf{\mbf{X}\middle|~\mbs{\theta}}\pdf{\mbs{\theta}}}{\pdf{\mbf{X}}}\\
            &= \argmax_{\mbs{\theta}} \pdf{\mbf{X}\middle|~\mbs{\theta}}\pdf{\mbs{\theta}}.
        \end{align}
\end{mydefinition}

\subsubsection{MAP estimator on Gaussian variables}

\section{The maximum a-posteriori estimator}
\begin{center}
    \Huge
    \textbf{TO BE UPDATED}
    
    \textcolor{red}{
    Double check the terminology in this chapter!
    }
\end{center}

In this section, the MAP estimator for normally distributed variables will be derived. For each problem, a process model is provided in the form 
\begin{align}
\mbf{x}_{k+1} &= \mbf{f}(\mbf{x}_{k}, \mbf{u}_{k}, \mbfuline{w}_{k}),
\end{align}
where $\mbfuline{w}_{k}\sim\mc{N}(\mbf{0},\mbs{\Sigma}_{\mbf{w}})$ is the Gaussian process noise.
Further, a measurement model is given in the form
\begin{align}
\mbf{y}_{k} &= \mbf{g}(\mbf{x}_{k}, \mbfuline{v}_{k}),
\end{align}
where $\mbfuline{v}_{k}\sim\mc{N}(\mbf{0},\mbs{\Sigma}_{\mbf{v}})$ is the Gaussian measurement noise. 

The goal is to estimate the states $\mbf{x}_{1:K}$ given the measurements and their distributions $\mbf{u}_{0:K-1}$, $\mbf{y}_{1:K}$, and the prior on the initial state $\mbf{x}_{0}\sim\mc{N}(\mbfhat{x}_{0},\mbfhat{P}_{0})$. To derive the maximum a-posteriori (MAP) estimator, the likelihood of $\mbf{x}_{1:K}$ must first be derived. 

The posterior of $\mbf{x}_{1:K}$ is given by
\begin{align}
\ell\left(\mbf{x}_{1:K}; \mbf{u}_{0:K-1}, \mbf{y}_{1:K}, \mbfhat{x}_{0}\right)
&= \pdf{\mbf{x}_{1:K} \middle\vert~ \mbf{u}_{0:K-1}, \mbf{y}_{1:K}, \mbfhat{x}_{0}}\\
&= 
\f{
    \pdf{\mbf{y}_{1:K} \middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}, \mbf{x}_{1:K}}\pdf{\mbf{x}_{1:K}\middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}}
}
{
    \pdf{\mbf{y}_{1:K}\middle\vert~\mbf{u}_{0:K-1}, \mbfhat{x}_{0}}
}\\
&=
    \label{eq: LS derivation Bayes. Marginalizing x, finding eta}
    \f{
        \pdf{\mbf{y}_{1:K} \middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}, \mbf{x}_{1:K}}\pdf{\mbf{x}_{1:K}\middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}}
    }
    {
        \int_{-\infty}^{\infty} \pdf{\mbf{y}_{1:K}\middle\vert~\mbf{u}_{0:K-1}, \mbfhat{x}_{0}, \mbf{x}_{1:K}}\pdf{\mbf{x}_{1:K}}\dee\mbf{x}_{1:K}
    }\\
&= 
    \eta \pdf{\mbf{y}_{1:K} \middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}, \mbf{x}_{1:K}}\pdf{\mbf{x}_{1:K}\middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}}\\
&= 
    \label{eq: LS derivation Bayes. y independent of u}
    \eta \pdf{\mbf{y}_{1:K} \middle\vert~ \mbf{x}_{1:K}}\pdf{\mbf{x}_{1:K}\middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}}\\
&= 
    \label{eq: LS derivation Bayes. y independent of ys}
    \eta \prod_{k=1}^{M}\pdf{\mbf{y}_{k} \middle\vert~ \mbf{x}_{k}}\pdf{\mbf{x}_{1:K}\middle\vert~ \mbf{u}_{0:K-1}, \mbfhat{x}_{0}}\\
&= 
    \label{eq: LS derivation Bayes. Markov prop}
    \eta \prod_{k=1}^{M}\pdf{\mbf{y}_{k} \middle\vert~ \mbf{x}_{k}}
    \prod_{k=1}^{K}\pdf{\mbf{x}_{k}\middle\vert~ \mbf{x}_{k-1}, \mbf{u}_{k-1}}\pdf{\mbf{x}_{0}}\\
&= 
    \label{eq: LS derivation Bayes. Reorganize}
    \eta
    \pdf{\mbf{x}_{0}}
    \prod_{k=1}^{K}\pdf{\mbf{x}_{k}\middle\vert~ \mbf{x}_{k-1}, \mbf{u}_{k-1}}
    \prod_{k=1}^{M}\pdf{\mbf{y}_{k} \middle\vert~ \mbf{x}_{k}}\\
&= 
    \label{eq: LS derivation Bayes. Expand with exponential}
    \eta
    \gaussian{\mbf{x}_{0}}{\mbfhat{x}_{0}}{\mbfhat{P}_{0}}
    \prod_{k=1}^{K}\gaussian{\mbf{x}_{k}}{\mbf{f}\left(\mbf{x}_{k-1}, \mbf{u}_{k-1}\right)}{\mbs{\Sigma}_{\mbf{w}}}
    \prod_{k=1}^{M}\gaussian{\mbf{y}_{k}}{\mbf{g}\left(\mbf{x}_{k}\right)}{\mbs{\Sigma}_{\mbf{v}}},
\end{align}
where $p(\cdot)$ is the probability distribution function (PDF) of a random variable and $\eta$ is the normalizing coefficient given in the denominator of \eqref{eq: LS derivation Bayes. Marginalizing x, finding eta}. Equation~\eqref{eq: LS derivation Bayes. y independent of u} is obtained by taking into account that $\mbf{y}_{k}$ is independent of $\mbf{u}_{k}$ for all $k$, \eqref{eq: LS derivation Bayes. y independent of ys} is obtained because $\mbf{y}_{j}$ is independent of $\mbf{y}_{i}$ for $i\neq j$, \eqref{eq: LS derivation Bayes. Markov prop} exploits the Markov property of the process model, and \eqref{eq: LS derivation Bayes. Expand with exponential} is the Gaussian PDF written out.

Taking the negative log of \eqref{eq: LS derivation Bayes. Expand with exponential} gives
\begin{align}
    \label{eq: LS derivation. -Log likelihood}
    L\left(\mbf{x}_{1:K};  \mbf{u}_{0:K-1}, \mbf{y}_{1:K}, \mbfhat{x}_{0}\right) &= 
    -\log\ell\left(\mbf{x}_{1:K};  \mbf{u}_{0:K-1}, \mbf{y}_{1:K}, \mbfhat{x}_{0}\right)\\
    &=
    \eta \left(
    \f{1}{2} \norm{\mbf{x}_{0}-\mbfhat{x}_{0}}_{\mbfhat{P}_{0}\inv}^{2} +
    \f{1}{2}\sum_{k=1}^{K}\norm{\mbf{x}_{k} - \mbf{f}\left(\mbf{x}_{k-1}, \mbf{u}_{k-1}\right)}_{\mbs{\Sigma}_{\mbf{w}}\inv}^{2} \right.\\\nonumber&\qquad\left. + 
    \f{1}{2}\sum_{k=1}^{M}\norm{\mbf{y}_{k} - \mbf{g}\left(\mbf{x}_{k}\right)}_{\mbs{\Sigma}_{\mbf{v}}\inv}^{2}
    \right),
\end{align}
The \emph{maximum a-posteriori} (MAP) estimate, as the name suggests, finds the $\mbf{x}$ that maximizes the posterior \eqref{eq: LS derivation Bayes. Expand with exponential} which is \emph{equivalent} to minimizing the negative log \eqref{eq: LS derivation. -Log likelihood}. 
% However, the MLE is not desirable as it requires the computation of $\eta$ which is expensive. However, since
    Since $\eta$ does not depend on $\mbf{x}$, then it does not affect the optimization problem. Maximizing the negative-log posterior \eqref{eq: LS derivation. -Log likelihood} with respect to $\mbf{x}$ without computing $\eta$ gives the MAP estimator. Specifically,
\begin{align}
    \mbfhat{x}_{0:K, \mathrm{MAP}} &= \argmin_{\mbf{x}_{1:K}} \f{1}{2} \norm{\mbf{x}_{0}-\mbfhat{x}_{0}}_{\mbfhat{P}_{0}\inv}^{2} +
    \f{1}{2}\sum_{k=1}^{K}\norm{\mbf{x}_{k} - \mbf{f}\left(\mbf{x}_{k-1}, \mbf{u}_{k-1}\right)}_{\mbs{\Sigma}_{\mbf{w}}\inv}^{2} +
    \f{1}{2}\sum_{k=1}^{M}\norm{\mbf{y}_{k} - \mbf{g}\left(\mbf{x}_{k}\right)}_{\mbs{\Sigma}_{\mbf{v}}\inv}^{2}
\end{align}
which is a least squares problem.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conditional probability
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Passing measurements through a function}
\begin{mytheorem}
   [Passing measurements through a function]    
   Let $\mbfrv{y}\in\rnums^{m}$ be a (vector) random variable that depends on $\mbfrv{x}\in\rnums^{n}$ through
   \begin{align}
       \mbfrv{y} &= \mbf{g}\left( \mbfrv{x} \right).
   \end{align}
   Furthermore, let $\mbfrv{z}\in\rnums^{p}$ be a transformed random variable given by
   \begin{align}
       \mbfrv{z} &= \mbf{f}\left( \mbfrv{y} \right).
   \end{align}
   Then 
   \begin{align}
       \pdf{\mbfrv{x}\middle|~\mbf{y}, \mbf{z}} 
       &=
       \pdf{\mbfrv{x}\middle|~\mbf{y}}.
    \end{align}
    Furthermore, if $\mbf{g}$ is invertible and $\mbf{f}$ is invertible, then 
    \begin{align}
        \label{eq:conditional prob xyz eq xz}
        \pdf{\mbfrv{x}\middle|~\mbf{y}, \mbf{z}} 
        &=
        \pdf{\mbfrv{x}\middle|~\mbf{z}}.
    \end{align}
    Note that if $\mbf{g}$ is invertible but $\mbf{f}$ is non-invertible, then \eqref{eq:conditional prob xyz eq xz} does not hold in general!
\end{mytheorem}
\begin{proof}
    \begin{enumerate}
        \item The first part.
        \begin{align}
            \pdf{\mbf{x}\middle|~\mbf{y}, \mbf{z}} 
            &=
            \f{
                \pdf{\mbf{y}, \mbf{z}\middle|~\mbf{x}}\pdf{\mbf{x}}
            }{
                \pdf{\mbf{y}, \mbf{z}}
            }\\
            &= 
            \f{
                \pdf{\mbf{z}\middle|~\mbf{y},\mbf{x}}\pdf{\mbf{y}\middle|~\mbf{x}}\pdf{\mbf{x}}
            }{
                \pdf{\mbf{z}\middle|~\mbf{y}}\pdf{\mbf{y}}
            }\\
            &= 
            \f{
                \cancel{\pdf{\mbf{z}\middle|~\mbf{y}}}\pdf{\mbf{y}\middle|~\mbf{x}}\pdf{\mbf{x}}
            }{
                \cancel{\pdf{\mbf{z}\middle|~\mbf{y}}}\pdf{\mbf{y}}
            }\\
            &= 
                \f{
                    \pdf{\mbf{y}\middle|~\mbf{x}}\pdf{\mbf{x}}
                }{
                    \pdf{\mbf{y}}
                }\\
            &= \pdf{\mbf{x}\middle|~\mbf{y}}.
        \end{align}

        \item The second part. Let's expand $\pdf{\mbf{x}\middle|~\mbf{y},\mbf{z}}$.
        \begin{align}
            \pdf{\mbf{x}\middle|~\mbf{y}, \mbf{z}}
            &=
            \f{
                \pdf{\mbf{y},\mbf{z}\middle|~\mbf{x}}\pdf{\mbf{x}}
            }{
                \pdf{\mbf{y}, \mbf{z}}
            }\\
            &=
            \f{
                \pdf{\mbf{y}\middle|~\mbf{z},\mbf{x}}\pdf{\mbf{z}\middle|~\mbf{x}}\pdf{\mbf{x}}
            }{
                \pdf{\mbf{y}\middle|~\mbf{z}}\pdf{\mbf{z}}
            }\\
            &=
            \f{
                \pdf{\mbf{y}\middle|~\mbf{z},\mbf{x}}
            }{
                \pdf{\mbf{y}\middle|\mbf{z}}
            }
            \pdf{\mbf{x}\middle|~\mbf{z}}.
        \end{align}
        Thus, \eqref{eq:conditional prob xyz eq xz} satisfied if and only if
        \begin{align}
            \pdf{\mbf{y}\middle|~\mbf{z},\mbf{x}} &= \pdf{\mbf{y}\middle|\mbf{z}}.
        \end{align}
        That is, if $\mbf{y}$ is independent of $\mbf{x}$ \emph{given} $\mbf{z}$. More rigorously, if there exists $\mbf{y}_{1}\neq\mbf{y}_{2}$ such that
        \begin{align}
            \begin{aligned}
                \mbf{g}(\mbf{x}_{1}) &= \mbf{y}_{1}\\
                \mbf{g}(\mbf{x}_{2}) &= \mbf{y}_{2}\\
            \end{aligned}
            &\implies \mbf{x}_{1}\neq\mbf{x}_{2},
        \end{align}
        and
        \begin{align}
            \label{eq:conditional probl proof z=fy1=fy2}
            \mbf{z} = \mbf{f}\left( \mbf{y}_{1} \right) 
            = \mbf{f}\left( \mbf{y}_{2} \right).
        \end{align}
        Then knowing $\mbf{z}$ without knowing $\mbf{x}$ will result in a non-singleton sample space $S$ for the random variable $\mbfrv{y}$. That is, a set with more than one element which implies that $\mbfrv{y}$ cannot be determined uniquely. In such case, the sample space is the set of $\mbf{y}$ that satisfy \eqref{eq:conditional probl proof z=fy1=fy2}.

        On the other hand, if in addition to knowing $\mbf{z}$, $\mbf{x}$ is also known, then the sample space of $\mbf{y}$ is a singleton. That is, $\mbfrv{y}$ can be determined uniquely.

        The condition \eqref{eq:conditional probl proof z=fy1=fy2} occurs only if the function $\mbf{f}$ is non-invertible.
    \end{enumerate}
\end{proof}
